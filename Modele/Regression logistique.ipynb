{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération 0, Coût: 0.4030\n",
      "Itération 500, Coût: 0.1208\n",
      "Itération 1000, Coût: 0.1148\n",
      "Itération 1500, Coût: 0.1141\n",
      "Itération 2000, Coût: 0.1141\n",
      "Itération 2500, Coût: 0.1141\n",
      "Itération 3000, Coût: 0.1141\n",
      "Itération 3500, Coût: 0.1141\n",
      "meilleur theta: [-1.00980259e+00  3.24227588e-02  1.15574179e-02  3.43770858e-02\n",
      "  2.59039078e+00  1.24910836e-03  2.11033778e-03 -3.65855432e-03\n",
      "  7.74641072e-02  7.49778225e-03  2.98443533e-02 -1.31793169e-01\n",
      "  9.47418026e-03  3.30986014e-02  1.88079215e-02]\n",
      "\n",
      "Coefficients (intercept + coeffs normalisés): [-1.0098e+00  3.2400e-02  1.1600e-02  3.4400e-02  2.5904e+00  1.2000e-03\n",
      "  2.1000e-03 -3.7000e-03  7.7500e-02  7.5000e-03  2.9800e-02 -1.3180e-01\n",
      "  9.5000e-03  3.3100e-02  1.8800e-02]\n",
      "Prédictions decision: [ 0.32  0.09  0.58  0.59  0.04  0.59  0.12  0.57  0.61  0.78  0.17 -0.19\n",
      "  0.14  0.33  0.31  0.65  0.66  0.36  0.37  0.41  0.1   0.25  0.13  0.18\n",
      "  0.44  0.36  0.34  0.08  0.25  0.07 -0.19  0.14  0.14  0.24  0.32  0.24\n",
      "  0.3   0.59  0.6   0.1   0.36  0.45  0.16  0.32  0.35  0.72  0.82  0.16\n",
      "  0.42  0.18  0.36  0.38  1.66  0.27  0.07  0.24  0.37  0.58 -0.1   0.38\n",
      " -0.09  0.1   0.11  0.91  0.88  0.62 -0.15  0.84  0.58  0.14  0.6   0.14\n",
      "  0.38  0.54  1.31  0.53 -0.89  0.64  1.29  0.16  0.29  0.36  0.43  0.42\n",
      "  0.77  0.41  1.39  0.44  0.88  0.27  0.4   0.34  0.32  0.62  0.32  0.41\n",
      "  0.68  0.33  0.17  0.39  0.64  0.04  0.59  0.36  1.61  0.65  0.6   0.18\n",
      "  0.52  0.44  0.62  0.13  0.56  0.11  0.12  0.36 -0.96  0.57  0.33  0.16\n",
      "  0.38  0.33  0.3   0.34  0.08  0.6   0.62  0.33  0.33  0.66  0.08  0.83\n",
      "  0.53  1.39  0.42  0.33 -0.16  0.35  0.29  0.82  0.58  0.48  0.29  0.33\n",
      "  0.34  0.61  0.56  0.15  0.35  0.48  0.24  0.37  0.81  0.61 -0.43  0.41\n",
      " -0.13  0.36  0.37  0.3   1.06  0.33  0.55  0.06  0.58  0.21  0.1   0.06\n",
      "  0.52  0.14  0.36  1.16  0.35  0.23  1.41  0.63  0.37  0.33  0.11  0.44\n",
      "  0.32  0.35  0.4   0.33  0.36  1.14  0.53  0.32  0.68  0.37  0.35  0.41\n",
      "  0.87  0.88  1.11  0.32  0.58 -0.17  0.66  0.33  0.77  0.41  1.15  0.39\n",
      "  0.07 -0.14  0.08  0.59  0.35  0.67  0.35  0.91  0.28  0.59  0.09  0.1\n",
      "  0.12  0.58  0.6   0.54 -0.17  0.5   0.3   0.85  0.41  0.85  0.59  0.31\n",
      " -0.16 -0.45  0.13  0.15  0.32  0.41  0.26  1.23  0.26  0.18  0.13  0.31\n",
      "  0.48  0.35  0.73  0.12  0.51  0.07  0.42  0.24  0.26  1.3   0.32  0.37\n",
      "  1.41  0.4   0.35  0.94  0.23  0.62  0.47  0.35  0.49  0.36 -0.45  0.52\n",
      "  0.63  0.45 -0.22  0.61  0.56  0.35  0.38  0.83  0.15  0.36  0.18  0.22\n",
      "  0.38  0.41  0.32  0.42  0.1   0.51  0.6   0.4  -0.93 -0.12  0.4   0.54\n",
      "  0.12  0.15  0.36  0.34  0.18  0.47  0.19  0.35  0.35  0.05  0.76  0.87\n",
      "  0.16  0.09  0.1   0.85  0.22  0.14  0.65  0.65  0.38  0.59  0.08  0.74\n",
      "  0.08  0.63  0.07  0.38  0.12  0.55  0.22  0.89  0.94  0.47  0.27  0.36\n",
      "  1.    0.36  0.67  0.56  0.62 -0.17  0.34 -0.17  0.38  0.96  0.6   0.93\n",
      " -0.18  0.18  0.54  0.13  0.34  0.18  0.36  0.59 -0.17  0.6   0.25  0.45\n",
      "  0.93  0.54  0.33  0.34  0.56  0.26  0.11  0.64  0.65  0.22  0.36  0.91\n",
      " -0.85  0.43  0.49  0.18  0.36  0.45  0.32  0.32  0.34  0.7   0.4   0.77\n",
      "  0.64  0.47  0.29  0.45  0.16  0.48  0.63  0.28  0.71  1.27  1.27  0.36\n",
      "  0.35  1.36  0.84  0.76  0.22  1.63  1.39  0.89  0.32  0.33  0.35  0.66\n",
      "  0.36  0.88  0.09  0.37  0.23  0.63  0.34  0.74  0.72  0.63  0.91  0.33\n",
      "  0.06  0.37  0.34  0.15  0.38  0.85  0.6  -0.21  0.93  0.31  0.34  0.26\n",
      "  0.33  0.83  0.35  0.21  0.42  1.09  0.83  0.31  0.1   0.59  0.65  0.7\n",
      "  0.63  0.09  0.27  0.35  0.62  0.24  0.38  0.08  0.92  0.35  0.14  0.35\n",
      " -0.19  0.13  0.56  0.3   0.86  0.28  0.38  0.35  0.26  0.35  0.06  0.86\n",
      "  0.41  0.34  0.58  0.3  -0.17  0.6   0.73  0.72  0.65  0.47  0.11  0.32\n",
      "  0.56  0.59  0.39  0.32  0.07  0.12  0.57  0.35  0.18  0.04  0.1   0.32\n",
      "  0.34  0.37  0.57  1.12  0.59  0.37  0.1   0.41  0.35  0.67  0.1   0.05\n",
      "  0.46  1.37  0.3  -0.95  0.69  0.09  0.38  0.09  0.27  0.5   0.38  0.15\n",
      "  0.78  0.33  0.09  0.28 -0.18  0.77  0.1   0.55  0.42  0.87 -0.14  0.36\n",
      "  0.22  0.63  1.6   0.05  0.08  0.37  0.51  0.18  0.07  1.27  0.55  1.38\n",
      "  0.6   0.29  0.05  0.55  0.66  0.31  0.43 -0.15  0.06  0.18  0.1   0.89\n",
      "  1.21  0.19  0.63  0.53  0.55  0.39 -0.16  0.46  0.34  0.37  0.1   0.08\n",
      "  0.37  0.07  0.05  0.61  0.23  0.33  0.14  0.6  -0.19  0.37  0.59  0.1\n",
      "  0.36  0.37 -0.14  0.48  0.29  0.61  0.14  0.51  1.28  0.34  0.47  0.35\n",
      "  0.55  0.65  0.26  0.08  0.05  0.59  0.42 -0.16  0.49 -0.17  0.11  0.39\n",
      "  1.4   0.5   0.25  0.41  0.87  0.32  0.25  0.62  0.86 -0.17  0.91  0.37\n",
      "  0.16  0.24  0.84 -0.12  0.1   0.15  0.55  1.19  0.24  0.59  0.38  0.44\n",
      "  0.36  0.12  0.33  0.59  0.24  0.42  0.09  0.04  1.16  0.08  0.51  1.32\n",
      "  0.88  0.58  0.22  0.49  0.11  0.62  0.4   0.6   0.11  0.1   0.6   0.67\n",
      "  0.6  -0.19  0.9   1.15  0.25  0.43  0.32  0.35  0.2   0.35  0.81  0.33\n",
      "  0.35  0.08  0.43  0.31  0.4  -0.12  0.39 -0.42  0.08 -0.17  0.14  0.44\n",
      "  0.17  0.32  0.35  0.45  0.63  0.35  0.34  0.35  0.07  0.49  0.34  0.34\n",
      "  0.6   0.1   0.35  0.2   0.1   0.35  0.3   0.79  0.32  0.19  0.67  0.5\n",
      "  0.35  0.11  0.11  0.06  0.35  1.44  0.76  0.37  0.11  0.84  0.38  0.34\n",
      "  0.22  0.25 -0.09  0.35  0.34  0.57  0.32  0.58  0.5   1.    0.33  0.36\n",
      "  0.58  0.18  0.41  0.37  0.08  0.31  0.84  1.68  0.26 -0.12  0.07 -0.17\n",
      " -0.42  0.61  0.09  0.87  0.37  0.3   0.33  0.34  0.25  0.13  1.06  0.16\n",
      "  0.57  0.28  0.35  0.32  0.1   1.64  0.64  0.4   0.46  0.73  0.66  0.35\n",
      "  0.32 -0.45 -0.14  0.61  0.37  0.26  0.49  0.1   0.6   0.48  1.69  0.21\n",
      "  0.06  0.09  0.56  0.4   0.33  0.46  0.44  0.6   0.54  0.71  0.23  0.44\n",
      "  0.34  0.35  0.95  0.83  0.35  0.91  0.47  0.27  0.58  0.96  0.38  0.12\n",
      " -0.44  0.09  0.38  0.87  0.14  0.37 -0.97 -0.17  0.36  0.24  0.64  0.65\n",
      "  0.12  0.4   0.59  0.59  0.73  1.33  0.58  0.57  0.3   0.42  0.45  0.24\n",
      "  0.41  0.32  0.62  0.32  0.34  0.4   0.4   0.81  0.62  0.55  0.64  1.68\n",
      "  0.14  0.78 -0.43  0.08  0.07  0.1   0.73  0.66  0.07  0.85  0.38  1.32\n",
      "  0.24  0.93  0.35  0.33  0.45  0.62  0.35  1.36  0.5   1.11  0.07  0.6\n",
      "  0.13  0.53  0.86 -0.22  0.39 -0.1   0.32  0.1   0.06  0.35  0.33  1.42\n",
      "  0.94  0.41  0.16  0.41  0.35  0.08  1.38 -0.12  0.52  0.54  0.12  0.67\n",
      "  0.3   0.62  0.6   0.34  0.76  0.08  0.09  0.34  0.21  0.33  0.6   0.34\n",
      "  1.25  0.34  1.17  1.02  0.36  0.06  0.4   1.67  0.37  0.21 -0.16  0.65\n",
      "  0.35  0.08  0.12  0.27  0.6   0.13  0.1   0.34  0.33  0.57  0.62  0.61\n",
      "  0.11  0.34  0.88  0.35  0.73  0.09 -0.18  0.61  0.43  0.4  -0.42  0.22\n",
      "  0.34  0.1  -0.11  0.34  0.34  0.31  0.08  0.37  0.6   0.09  0.63 -0.46\n",
      "  0.33  0.59  0.36  0.18  0.34  0.43 -0.12  0.34 -0.41  0.35  0.07  0.39\n",
      "  0.11  0.56  0.38  0.13  0.13  0.37  0.18  0.33  0.19  0.52 -0.11  0.39\n",
      "  0.09  0.64  0.08 -0.04  0.08  0.61 -0.18  0.63  0.48  0.23  0.32  0.33\n",
      " -0.17  0.19  0.73  0.36  1.31  0.67  0.34  1.21 -0.04  0.98  1.38  0.44\n",
      "  0.1   0.87  0.08 -0.19  0.08  0.33  0.04 -0.19  0.39  0.58  0.6   0.84\n",
      "  0.26  0.38  0.27  1.35 -0.14  0.58  0.38  0.33  0.46  0.14 -0.15  0.32\n",
      "  0.84  0.53  0.65  0.4 ]\n",
      "Vraies valeurs Decision: [0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1\n",
      " 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0\n",
      " 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0\n",
      " 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0\n",
      " 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
      " 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1\n",
      " 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1\n",
      " 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1\n",
      " 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0\n",
      " 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0\n",
      " 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0\n",
      " 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1\n",
      " 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1\n",
      " 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
      " 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0\n",
      " 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
      " 0]\n",
      " MSE: 0.1141\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Exemple avec tes données (remplace par ton CSV)\n",
    "# df = pd.read_csv('synthetic_2data_1000.csv')\n",
    "# Pour test : données simulées\n",
    "\n",
    "#data = {\n",
    "#    'Study_Hours_Weekly': [10, 20, 5, 30, 15],\n",
    "#    'Class_Regularity': [6, 4, 5, 6, 3],  # sur 6\n",
    "#    'GPA': [12.5, 14.0, 11.0, 15.5, 10.5]\n",
    "#}\n",
    "\n",
    "#df = pd.DataFrame(data)\n",
    "df = pd.read_csv('synthetic_2data_1000.csv')\n",
    "\n",
    "X = df[['Age', 'Gender', 'Level', 'GPA', 'Teaching_Quality', 'Lab_Sessions', 'Structured_Plan','Living_Situation', 'Sleep_Hours_Daily', 'Physical_Activity', 'Success_Factors_Len', 'Improvement_Suggestions_Len','Study_Hours_Weekly', 'Class_Regularity']].values\n",
    "y = df['Decision'].values\n",
    "\n",
    "# Normalisation Min-Max (essentielle pour convergence rapide)\n",
    "X_norm = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-8)\n",
    "\n",
    "# Ajout du bias (intercept)\n",
    "X_b = np.c_[np.ones(X_norm.shape[0]), X_norm]\n",
    "\n",
    "# Descente de gradient\n",
    "def gradient_descent_linear(X, y, lr=0.1, n_iters=4000):\n",
    "    m = len(y)\n",
    "    theta = np.zeros(X.shape[1])  # Initialisation à zéro (mieux pour petite échelle)\n",
    "    costs = []\n",
    "    for i in range(n_iters):\n",
    "        preds = X.dot(theta)\n",
    "        errors = preds - y\n",
    "        gradients = (2/m) * X.T.dot(errors)\n",
    "        theta -= lr * gradients\n",
    "        cost = (1/m) * np.sum(errors**2)\n",
    "        costs.append(cost)\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Itération {i}, Coût: {cost:.4f}\")\n",
    "    return theta, costs\n",
    "\n",
    "theta_best, costs = gradient_descent_linear(X_b, y)\n",
    "\n",
    "# Prédictions (sur données normalisées, puis dénormaliser si besoin)\n",
    "print(\"meilleur theta:\", theta_best)\n",
    "preds = X_b.dot(theta_best)\n",
    "print(\"\\nCoefficients (intercept + coeffs normalisés):\", theta_best.round(4))\n",
    "print(\"Prédictions decision:\", preds.round(2))\n",
    "print(\"Vraies valeurs Decision:\", y)\n",
    "print(\" MSE:\", costs[-1].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération 0, Coût: 0.4125\n",
      "Itération 500, Coût: 0.1175\n",
      "Itération 1000, Coût: 0.1166\n",
      "Itération 1500, Coût: 0.1166\n",
      "Itération 2000, Coût: 0.1166\n",
      "Itération 2500, Coût: 0.1166\n",
      "Itération 3000, Coût: 0.1166\n",
      "Itération 3500, Coût: 0.1166\n",
      "Itération 4000, Coût: 0.1166\n",
      "meilleur theta: [-9.96224394e-01 -5.47584526e-06  6.42629990e-03  6.97143099e-02\n",
      "  2.61668944e+00  1.40072604e-02  6.17446174e-03 -1.04546298e-02\n",
      "  7.06629995e-02  1.20573404e-02  3.94695576e-02 -1.22741888e-01\n",
      " -9.13136431e-03  2.79745240e-02 -1.67583363e-02]\n",
      " MSE: 0.1166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Exemple avec tes données (remplace par ton CSV)\n",
    "# df = pd.read_csv('synthetic_2data_1000.csv')\n",
    "# Pour test : données simulées\n",
    "\n",
    "#data = {\n",
    "#    'Study_Hours_Weekly': [10, 20, 5, 30, 15],\n",
    "#    'Class_Regularity': [6, 4, 5, 6, 3],  # sur 6\n",
    "#    'GPA': [12.5, 14.0, 11.0, 15.5, 10.5]\n",
    "#}\n",
    "\n",
    "#df = pd.DataFrame(data)\n",
    "df = pd.read_csv('C:\\\\Users\\David Mechant\\\\Downloads\\\\synthetic_2data_1000.csv')\n",
    "\n",
    "x = df[['Age', 'Gender', 'Level', 'GPA', 'Teaching_Quality', 'Lab_Sessions', 'Structured_Plan','Living_Situation', 'Sleep_Hours_Daily', 'Physical_Activity', 'Success_Factors_Len', 'Improvement_Suggestions_Len','Study_Hours_Weekly', 'Class_Regularity']].values\n",
    "y = df['Decision'].values\n",
    "X_train = x[:800]\n",
    "y_train = y[:800]\n",
    "X_test = x[800:]\n",
    "y_test = y[800:]\n",
    "\n",
    "# Normalisation Min-Max (essentielle pour convergence rapide)\n",
    "X_norm = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0) + 1e-8)\n",
    "\n",
    "# Ajout du bias (intercept)\n",
    "X_b = np.c_[np.ones(X_norm.shape[0]), X_norm]\n",
    "\n",
    "# Descente de gradient\n",
    "def gradient_descent_linear(X, y, lr=0.19, n_iters=4001):\n",
    "    m = len(y)\n",
    "    theta = np.zeros(X.shape[1])  # Initialisation à zéro (mieux pour petite échelle)\n",
    "    costs = []\n",
    "    for i in range(n_iters):\n",
    "        #Met a jour le planificateur\n",
    "        preds = X.dot(theta)\n",
    "        errors = preds - y\n",
    "        gradients = (2/m) * X.T.dot(errors)\n",
    "        theta -= lr * gradients\n",
    "        cost = (1/m) * np.sum(errors**2)\n",
    "        costs.append(cost)\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Itération {i}, Coût: {cost:.4f}\")\n",
    "    return theta, costs\n",
    "\n",
    "theta_best, costs = gradient_descent_linear(X_b, y_train)\n",
    "\n",
    "# Prédictions (sur données normalisées, puis dénormaliser si besoin)\n",
    "print(\"meilleur theta:\", theta_best)\n",
    "preds = X_b.dot(theta_best)\n",
    "#print(\"\\nCoefficients (intercept + coeffs normalisés):\", theta_best.round(4))\n",
    "#print(\"Prédictions decision:\", preds.round(2))\n",
    "#print(\"Vraies valeurs Decision:\", y)\n",
    "print(\" MSE:\", costs[-1].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48877832 0.5307135  0.54062901 0.47506602 0.52845741 0.51094574\n",
      " 0.58986188 0.49599783 0.50607769 0.52521189 0.52349539 0.64653804\n",
      " 0.59540341 0.56462047 0.59740227 0.84939646 0.44858885 0.63803316\n",
      " 0.27834741 0.42640959 0.42526696 0.43400941 0.61623437 0.59619995\n",
      " 0.42797878 0.66581178 0.51476139 0.78484202 0.47432439 0.67604858\n",
      " 0.52053312 0.49898742 0.53728822 0.58471163 0.52000314 0.7850262\n",
      " 0.55317521 0.73455868 0.42130806 0.5804003  0.43591081 0.55984281\n",
      " 0.66245995 0.34439054 0.52013158 0.35994277 0.50639196 0.4281626\n",
      " 0.41754951 0.51232755 0.49511903 0.79910096 0.6879189  0.5265112\n",
      " 0.44169872 0.52329013 0.50611039 0.42536563 0.79691397 0.35764288\n",
      " 0.55321136 0.5656835  0.43452628 0.60834264 0.49119726 0.58808639\n",
      " 0.58663096 0.51296808 0.62865576 0.42122471 0.43570308 0.50900484\n",
      " 0.46342645 0.5039504  0.58532474 0.50308935 0.76142422 0.50403362\n",
      " 0.74629062 0.70461185 0.51416194 0.41376141 0.52295861 0.84758541\n",
      " 0.51374894 0.45771165 0.35195106 0.60222529 0.51380889 0.42808468\n",
      " 0.43340499 0.4873635  0.58659137 0.44352088 0.43392987 0.50919281\n",
      " 0.50023178 0.57700364 0.58088499 0.58692187 0.43464214 0.51061462\n",
      " 0.66859264 0.50303429 0.6235467  0.42584236 0.34691979 0.58581621\n",
      " 0.52703151 0.52079638 0.28220583 0.45979705 0.50634527 0.43069761\n",
      " 0.36117882 0.51173682 0.50656022 0.50150409 0.43119269 0.51598765\n",
      " 0.58033141 0.42823878 0.58974537 0.27469012 0.50591356 0.58014738\n",
      " 0.51206296 0.45269996 0.50621042 0.54504129 0.3602254  0.49942716\n",
      " 0.28512671 0.51120257 0.42009135 0.51260515 0.43892328 0.57184897\n",
      " 0.52229995 0.43955667 0.43952081 0.51620494 0.44946662 0.49803241\n",
      " 0.45570136 0.56117499 0.36540111 0.51737282 0.43094801 0.59317691\n",
      " 0.42765744 0.3986565  0.42604199 0.58658065 0.34613903 0.60027379\n",
      " 0.5540122  0.46983155 0.49849473 0.49684419 0.35587621 0.45230135\n",
      " 0.62957324 0.51416909 0.77697946 0.60711871 0.50784247 0.75483091\n",
      " 0.39692441 0.6899284  0.7957828  0.53838131 0.43567963 0.66425171\n",
      " 0.4238328  0.3462392  0.42699352 0.49792732 0.41282102 0.34842566\n",
      " 0.51800292 0.57382548 0.58992448 0.65874488 0.47367603 0.51435543\n",
      " 0.486906   0.77840678 0.35891872 0.58193728 0.52482223 0.50962426\n",
      " 0.53767615 0.44135851 0.35811979 0.49755233 0.659316   0.56673819\n",
      " 0.60403779 0.52126423]\n",
      "[0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0\n",
      " 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1]\n",
      "[0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1\n",
      " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0\n",
      " 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#test apres entrqine;ent du modele\n",
    "# Normalisation Min-Max (essentielle pour convergence rapide)\n",
    "X_test_norm = (X_test - X_test.min(axis=0)) / (X_test.max(axis=0) - X_test.min(axis=0) + 1e-8)\n",
    "\n",
    "# Ajout du bias (intercept)\n",
    "X_test_b = np.c_[np.ones(X_test_norm.shape[0]), X_test_norm]\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -250, 250)))  # Évite overflow\n",
    "\n",
    "def predict_sigmoid(X,theta):\n",
    "    z=np.dot(X,theta)\n",
    "    return sigmoid(z)\n",
    "\n",
    "pred = predict_sigmoid(X_test_b,theta_best)\n",
    "prediction= (pred >= 0.5).astype(int)\n",
    "print(pred)\n",
    "print(prediction)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de reussite:76.50%\n"
     ]
    }
   ],
   "source": [
    "#teux de reussite dans la prediction\n",
    "accuracy = np.mean(prediction == y_test)\n",
    "print(f\"Taux de reussite:{accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scocre : 75.6477 %\n"
     ]
    }
   ],
   "source": [
    "#F1_score histoire de s'assurer du taux de predictiondu modele\n",
    "def f1_score(y,y_predi):\n",
    "    TP = sum((yt==1 and yp==1) for yt,yp in zip(y,y_predi))\n",
    "    FP = sum((yt==0 and yp==1) for yt,yp in zip(y,y_predi))\n",
    "    FN = sum((yt==1 and yp==0) for yt,yp in zip(y,y_predi))\n",
    "    precision = TP / (TP + FP) if (TP + FP)>0 else 0\n",
    "    rappel = TP / (TP + FN) if (TP + FN)>0 else 0\n",
    "    \n",
    "    if precision + rappel == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * rappel)/(precision + rappel)\n",
    "\n",
    "print(\"F1 scocre :\",round(f1_score(y_test,prediction)*100, 4),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
